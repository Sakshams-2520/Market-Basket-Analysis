# -*- coding: utf-8 -*-
"""Market Basket.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hemrH_g8r6xbsDnLgGJI_XEmHZn-tGJv

# Market Basket Analysis

**What is Market Basket Analysis ?**

Market Basket Analysis is one of the key techniques used by large retailers to uncover associations between items.Â 
It works by looking for combinations of items that occur together frequently in transactions. To put it another way, it allows retailers to identify relationships between the items that people buy.

# Market Basket Analysis For Foodmart Store Dataset

**About Foodmart**

![o.jpg](attachment:o.jpg)

Food Mart (FM) is a chain of convenience stores in the United States. The private company's headquarters are located in Mentor, Ohio, and there are currently approximately **325 stores located in the US**. Food Mart operates on the franchise system.

**Importing Libraries**
"""

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt

## Get multiple outputs in the same cell
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

## Ignore all warnings
import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings(action='ignore', category=DeprecationWarning)

## Display all rows and columns of a dataframe instead of a truncated version
from IPython.display import display
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

"""**Loading Customers Dataset**"""

customer=pd.read_csv("/Users/shraddhasurana/Desktop/projects/ProjectPro/MarketBasket/data/customer.csv")

customer.head()

"""**Loading Products Dataset**"""

product=pd.read_csv("../data/product.csv")

product.head()

"""**Loading Departments Dataset**"""

product_class=pd.read_csv("../data/product_class.csv")

product_class.head()

"""**Loading Region Dataset**"""

region=pd.read_csv("../data/region.csv")

region.head()

"""**Loading Sales Dataset**"""

df=pd.read_csv("../data/sales.csv")

df.head()

"""**Loading Stores Dataset**"""

store=pd.read_csv("../data/store.csv")

store.head()

"""**Loading Time by Day Dataset**"""

time_by_day=pd.read_csv("../data/time_by_day.csv")

time_by_day.head()

time_by_day.tail()

"""**Merging Customer Dataset in df Dataframe**"""

df=df.merge(customer,on='customer_id')

df.head()

"""**Merging Products Dataset in df Dataframe**"""

df=df.merge(product,on='product_id')

df.head()

"""**Merging Department Dataset in df Dataframe**"""

df=df.merge(product_class,on='product_class_id')

df.head()

"""**Merging Stores Dataset in df Dataframe**"""

df=df.merge(store,on='store_id')

df.head()

"""**Merging Region Dataset in df Dataframe**"""

df=df.merge(region,on='region_id')

df.head()

"""**Merging Time by Day Dataset in df Dataframe**"""

df=df.merge(time_by_day,on='time_id')

df.head()

"""**Converting Dataframe to Final Foodmart Offline Dataset**"""

df.to_csv("../data/Foodmart_dataset.csv")

"""# Exploratory Data Analysis (EDA)

**Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
import squarify
import networkx as nx
import warnings
import matplotlib as mpl
import gapminder as gapminder


# %matplotlib inline

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

"""**Loading Foodmart Offline Dataset**"""

df=pd.read_csv("../data/Foodmart_dataset.csv")

"""**Size of Foodmart Dataset**"""

df.shape

df.head()

"""# Data Exploration

**Describe**
"""

df.describe()

df.describe(include='all')

"""**Missing Values**"""

df.isnull().sum()

"""**Datatypes**"""

df.dtypes

"""**Checking Datatypes, Mising Value, Unique Value**"""

temp = pd.DataFrame(index=df.columns)
temp['data_type']=df.dtypes
temp['null_count']=df.isnull().sum()
temp['unique_count']=df.nunique()

temp

"""# Univariate Analysis

**Histograms**

**Frequency Plot Of Department Id**
"""

fig=plt.figure(figsize=(15,10))
df['product_class_id'].plot.hist(bins = 50)
plt.xlabel('department id')

"""**Frequency Plot Of Product Id**"""

fig=plt.figure(figsize=(15,10))
df['product_id'].plot.hist(bins = 100)
plt.xlabel('product id')

"""**Frequency Plot Of Unit Sales**"""

fig=plt.figure(figsize=(15,10))
df['unit_sales'].plot.hist(bins = 25)
plt.xlabel('Unit Sales')

"""**When do people order**

**Year**
"""

fig=plt.figure(figsize=(12,10))
df['the_year'].plot.hist(bins=10)
plt.xlabel('year')
plt.xticks([1996,1997,1998,1999])

"""**Month**"""

fig=plt.figure(figsize=(15,10))
df['the_month'].plot.hist(bins=50)
plt.xlabel('Month')

"""**Day Of Month**"""

fig=plt.figure(figsize=(15,10))
df['day_of_month'].plot.hist(bins=75)
plt.xlabel('Day of month')

"""**Day Of Week**"""

df_day_freq=df['the_day'].value_counts()
fig=plt.figure(figsize=(15, 10))
df_day_freq.plot.bar()

"""### Top choices

**Top 10 First Choices in Products**
"""

df['products'] = 'Products'
products = df.truncate(before = 605, after = 615)

products = nx.from_pandas_edgelist(products, source = 'products', target = 'product_name', edge_attr = True)

products

warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize']=(20,20)
pos=nx.spring_layout(products)
color=plt.cm.Reds(np.linspace(0,15,1))
nx.draw_networkx_nodes(products,pos,node_size=15000,node_color=color)
nx.draw_networkx_edges(products, pos, width = 3, alpha = 0.6, edge_color = 'black')
nx.draw_networkx_labels(products, pos, font_size = 20)
plt.axis('off')
plt.grid()
plt.title('Top 10 First Choices in Products', fontsize = 40)
plt.show()

"""**Top 10 First Choices in Department**"""

df['departments'] = 'Departments'
departments = df.truncate(before = 150, after = 195)

departments = nx.from_pandas_edgelist(departments, source = 'departments', target = 'department', edge_attr = True)

warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize']=(20,20)
pos=nx.spring_layout(departments)
color=plt.cm.Blues(np.linspace(0,15,1))
nx.draw_networkx_nodes(departments,pos,node_size=15000,node_color=color)
nx.draw_networkx_edges(departments, pos, width = 3, alpha = 0.6, edge_color = 'black')
nx.draw_networkx_labels(departments, pos, font_size = 20)
plt.axis('off')
plt.grid()
plt.title('Top 10 First Choices in Departments', fontsize = 40)
plt.show()

"""**Highest Ordered**

**Most Ordered Products**
"""

df['product_name'].value_counts()

"""**Most Ordered Products in Percentage**"""

df['product_name'].value_counts()/len(df)*100

"""**Most Visited Departments**"""

df['department'].value_counts()

"""**Most Visited Departments in Percentage**"""

df['department'].value_counts()/len(df)*100

"""**Most Visited Aisle**"""

df['subcategory'].value_counts()

"""**Most Visited Aisle in Percentage**"""

df['subcategory'].value_counts()/len(df)*100

"""**BarPlot**

**BarPlot of Most Visied Aisle**
"""

df_subcategory_freq=df['subcategory'].value_counts().iloc[:50]
fig=plt.figure(figsize=(15,10))
df_subcategory_freq.plot.bar()

"""**BarPlot of Most Visited Department**"""

fig=plt.figure(figsize=(15,10))
df['department'].value_counts().plot(kind='bar')

"""**BarPlot of Most Bought Product**"""

df_freq_products=df['product_name'].value_counts().iloc[:50]
fig=plt.figure(figsize=(15, 10))
df_freq_products.plot.bar()

"""**Array of Most Bought Product**"""

y=df_freq_products.head(50).to_frame()
y.index

"""**TreeMap for Most Bought Products**"""

plt.rcParams['figure.figsize']=(20,20)
color=plt.cm.cool(np.linspace(0,1,50))
squarify.plot(sizes=y.values,label=y.index,alpha=0.8,color=color)
plt.title('tree map for frequent products')
plt.axis('off')

"""# Data Manipulation"""

df.shape

"""**Drop Duplicates**"""

df.drop_duplicates()

df.shape

"""**Missing Values**"""

df.isnull().sum()

"""**Datatypes**"""

df.dtypes



"""# Bivariate Analysis

**Bar Plot**

**Bar Plot between customers and their products per order**
"""

data_user_orders_num=df.groupby('customer_id')['unit_sales'].count()
data_user_orders_num

source_data = {}
for i in range(10):
    source_data[str(10*i)+'~'+str(10*(i+1))]=len([x for x in list(data_user_orders_num) if x>=i*10 and x<(i+1)*10])
    
source_data

font_size=10
fig_size=(8,6)    
mpl.rcParams['font.size']=font_size
mpl.rcParams['figure.figsize']=fig_size
bar_width=0.3

 
x_axis = tuple(source_data.keys())
y_axis = tuple(source_data.values())
#assign color
plt.bar(x_axis, y_axis, color='rgb')  
# descrpitions for x-axis, y-axis
plt.xlabel('Unit sales')  
plt.ylabel("No. of customers") 
plt.title("Orders Scatter Plot") 
plt.show()

"""**Transaction ID** - create transaction id which denotes a basket"""

df['transaction_id'] = df['customer_id'].astype(str) + df['time_id'].astype(str)

df.head()

"""**Filtering out Columns**"""

cols = [77,3,1,24]
order_products=df[df.columns[cols]]

order_products.head()

"""**Average products bought by customers per order**"""

data_user_products_num1=order_products.groupby('transaction_id')['product_id'].count()
data_user_products_num1=pd.DataFrame(data_user_products_num1)
data_user_products_num1['transaction_id']=list(data_user_products_num1.index)
data_user_products_num1.columns=['product_num','orderid']
data_user_products_num2=pd.merge(data_user_products_num1,df[['transaction_id','customer_id']],on='transaction_id',how='left')

data_user_products_num3=data_user_products_num2.groupby('customer_id')['product_num'].agg(['sum','count'])
data_user_products_num3['avg']=data_user_products_num3['sum']/data_user_products_num3['count']

data_user_products_num3.head()

"""# Featured Products Department Wise"""

cols = [1,36,38,24]
departments=df[df.columns[cols]]
departments.head()



"""**List Of Departments**"""

temp=['department']
for i in temp:
    print('@@@@@@Value Count in',i,'@@@@@@@@@')
    print(df[i].value_counts())

"""**Produce Department**"""

produce=departments.loc[df['department'] == 'Produce']
produce.head()

"""**Featured Products in Produce Department**"""

top_produce=produce['product_name'].value_counts().iloc[:10]
top_produce.head()

"""**Featured Products in Snack Foods Department**"""

snacks=df.loc[df['department']=='Snack Foods']
top_snacks=snacks['product_name'].value_counts().iloc[:10]
top_snacks.head()

"""**Featured Products in HouseHold Department**"""

household=df.loc[df['department']=='Household']
top_household=household['product_name'].value_counts().iloc[:10]
print(top_household)

"""**Featured Products in Frozen Foods Department**"""

frozen=df.loc[df['department']=='Frozen Foods']
top_frozen=frozen['product_name'].value_counts().iloc[:10]
print(top_frozen)

"""**Featured Products in Baking Goods Department**"""

baking=df.loc[df['department']=='Baking Goods']
top_baking=baking['product_name'].value_counts().iloc[:10]
print(top_baking)

"""**Featured Products in Canned Foods Department**"""

canned=df.loc[df['department']=='Canned Foods']
top_canned=canned['product_name'].value_counts().iloc[:10]
print(top_canned)

"""**Featured Products in Dairy Department**"""

dairy=(df.loc[df['department'] == 'Dairy'])
top_dairy=dairy['product_name'].value_counts().iloc[:10]
print(top_dairy)

"""**Featured Products in Health and Hygiene Department**"""

dairy=(df.loc[df['department'] == 'Health and Hygiene'])
top_dairy=dairy['product_name'].value_counts().iloc[:10]
print(top_dairy)

"""**Featured Products in Beverages Department**"""

dairy=(df.loc[df['department'] == 'Beverages'])
top_dairy=dairy['product_name'].value_counts().iloc[:10]
print(top_dairy)

"""**Featured Products in Deli Department**"""

dairy=(df.loc[df['department'] == 'Deli'])
top_dairy=dairy['product_name'].value_counts().iloc[:10]
print(top_dairy)

"""**Featured Products in Alcoholic Beverages Department**"""

dairy=(df.loc[df['department'] == 'Alcoholic Beverages'])
top_dairy=dairy['product_name'].value_counts().iloc[:10]
print(top_dairy)

"""**Featured Products in Starchy Foods Department**"""

dairy=(df.loc[df['department'] == 'Starchy Foods'])
top_dairy=dairy['product_name'].value_counts().iloc[:10]
print(top_dairy)

"""**Featured Products in Eggs Department**"""

dairy=(df.loc[df['department'] == 'Eggs'])
top_dairy=dairy['product_name'].value_counts().iloc[:10]
print(top_dairy)

"""# Market Basket Analysis

![market-basket-analysis.png](attachment:market-basket-analysis.png)

**Importing Libraries**
"""

import pandas as pd 
import numpy as np 
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

"""**Filtering the Columns**"""

cols = [77,3,1,24,7,2]
product_name=df[df.columns[cols]]

product_name.head()

"""**Counting each product** The number of transactions a product appeared in"""

productCountDf = product_name.groupby("product_id",as_index = False)['transaction_id'].count()

productCountDf.head()

"""**Arranging Top Products**"""

productCountDf = productCountDf.sort_values("transaction_id",ascending = False)

productCountDf.head()

"""**Top 100 most frequently purchased products**"""

topProdFrame = productCountDf.iloc[0:100,:]
productId= topProdFrame.loc[:,["product_id"]]

topProdFrame

"""**Orders containting the the most frequently purchased products**"""

MarketBasketdf = product_name[0:0]
for i in range(0,99):
    pId = productId.iloc[i]['product_id'] 
    stDf = product_name[product_name.product_id == pId ]
    MarketBasketdf = MarketBasketdf.append(stDf,ignore_index = False)

MarketBasketdf.head()

"""**Putting the items into 1 transaction**"""

basket = MarketBasketdf.groupby(['transaction_id','product_name'])['unit_sales'].sum().unstack().reset_index().fillna(0).set_index('transaction_id')

basket

"""# One Hot Encoding

**Converted the units into 1 encoded value**
"""

def encode_units(x):
    if x <= 0:
        return 0
    if x >= 1:
        return 1

basket_sets = basket.applymap(encode_units)

basket_sets.head()

"""**Size and shape of basket**"""

basket_sets.size

basket_sets.shape

dummy=basket_sets.head(10000)

"""# Apriori Algorithm

**Importing Apriori and Association rules Libraries**
"""

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

"""**Frequent items with support 0.01% using Apriori Algorithm**"""

frequent_itemsets = apriori(dummy, min_support=0.0001, use_colnames=True)

frequent_itemsets

"""**Association rules using Apriori Algorithm**"""

apriori_rules = association_rules(frequent_itemsets, metric="lift", min_threshold= 1)

apriori_rules

"""**Filtering out co-realted products with higher Probability**"""

apriori_rules[ (apriori_rules['lift'] >= 50) & (apriori_rules['confidence'] >= 0.01) ]

"""# Recommendations using Apriori Algorithm"""

def recommendations_using_Apriori(item):
    recommend = []
    for i in range(0,2646):
        if item == apriori_rules.iloc[i,0]:
            recommend.append(apriori_rules.iloc[i,1])
    
    return recommend

"""**5 Recommendations with Better Chicken Noodle Soup**"""

product_name = {'Better Chicken Noodle Soup'}
recommentations=recommendations_using_Apriori(product_name)
print(*recommentations[0:5], sep = "\n")

"""**10 Recommendations with Moms Potato Salad**"""

product_name = {'Moms Potato Salad'}
recommentations=recommendations_using_Apriori(product_name)
print(*recommentations[0:10], sep = "\n")

"""**15 Recommendations with Carrington Ice Cream Sandwich**"""

product_name = {'Carrington Ice Cream Sandwich'}
recommentations=recommendations_using_Apriori(product_name)
print(*recommentations[0:15], sep = "\n")

"""# Fpgrowth Algorithm

**Importing Fpgrowth Libraries**
"""

from mlxtend.frequent_patterns import fpgrowth

"""**Frequent Items with support 0.001% using Fpgrowth Algorithm**"""

freq_items=fpgrowth(dummy,min_support=.0001,use_colnames=True)

freq_items

"""**Association Rules using Fpgrowth Algorithm**"""

fpgrowth_rules=association_rules(freq_items,metric="lift",min_threshold=1)

fpgrowth_rules

"""# Recommendations using Fpgrowth Algorithm"""

def recommendations_using_Fpgrowth(item):
    recommend = []
    for i in range(0,2646):
        if item == fpgrowth_rules.iloc[i,0]:
            recommend.append(fpgrowth_rules.iloc[i,1])
    
    return recommend

"""**5 Recommendations with Better Chicken Noodle Soup**"""

product_name = {'Better Chicken Noodle Soup'}
recommentations=recommendations_using_Fpgrowth(product_name)
print(*recommentations[0:5], sep = "\n")

"""**10 Recommendations with Moms Potato Salad**"""

product_name = {'Moms Potato Salad'}
recommentations=recommendations_using_Fpgrowth(product_name)
print(*recommentations[0:10], sep = "\n")

"""**15 Recommendations with Carrington Ice Cream Sandwich**"""

product_name = {'Carrington Ice Cream Sandwich'}
recommentations=recommendations_using_Fpgrowth(product_name)
print(*recommentations[0:15], sep = "\n")

"""# Apriori VS fpgrowth Algorithm

**Calculating Run Time of Apriori Algorithm**
"""

import time
l=[0.01,0.02,0.03,0.04,0.05]
t=[]
for i in l:
    t1=time.time()
    apriori(dummy,min_support=i,use_colnames=True)
    t2=time.time()
    t.append((t2-t1)*1000)

"""**Calculating Run Time of Fpgrowth Algorithm**"""

l=[0.01,0.02,0.03,0.04,0.05]
f=[]
for i in l:
    t1=time.time()
    fpgrowth(dummy,min_support=i,use_colnames=True)
    t2=time.time()
    f.append((t2-t1)*1000)

"""**Graph of Run Time between Apriori and Fpgrowth Algorithm**"""

sns.lineplot(x=l,y=f,label="fpgrowth")
sns.lineplot(x=l,y=t,label="apriori")
plt.xlabel("Min_support Threshold")
plt.ylabel("Run Time in ms")

"""![gg.png](attachment:gg.png)"""

